{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_ANNQ1.2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "cY_lToASPm8d",
        "outputId": "532ea038-3a13-45fc-db76-5deb04353a4a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/psantul/Dataset/main/data.csv\", encoding='latin-1')\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569, 33)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302         M  ...                  0.11890          NaN\n",
              "1    842517         M  ...                  0.08902          NaN\n",
              "2  84300903         M  ...                  0.08758          NaN\n",
              "3  84348301         M  ...                  0.17300          NaN\n",
              "4  84358402         M  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2g_BJpWaXRb"
      },
      "source": [
        "del data['Unnamed: 32']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6JC1Mz8ad3C"
      },
      "source": [
        "x = data.iloc[:,2:].values\n",
        "y = data.iloc[:,1].values"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hfncJqhan-w"
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtGGD6MGbCvJ"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSQCSAE5bTvz"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.fit_transform(X_test)\n",
        "acc_list=[]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaBEp4UZR3Bu"
      },
      "source": [
        "class NeuralNetwork_sigmoid:\n",
        "    \n",
        "    def __init__(self,X, y, X_test, y_test, hidden_nodes=12, learning_rate=0.1, epochs=5000):\n",
        "        \n",
        "        #data\n",
        "        self.y = y[:,None]\n",
        "        self.X = X\n",
        "        \n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "\n",
        "        #parameters\n",
        "        np.random.seed(4)\n",
        "        self.input_nodes = len(X[0])\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = self.y.shape[1]\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        #init weights\n",
        "        self.w1 = 2*np.random.random((self.input_nodes, self.hidden_nodes)) - 1\n",
        "        self.w2 = 2*np.random.random((self.hidden_nodes, self.output_nodes)) - 1\n",
        "\n",
        "        self.train(epochs)\n",
        "        self.test()\n",
        "        \n",
        "    def sigmoid(self,X):\n",
        "        return (1/(1+np.exp(-X)))\n",
        "    \n",
        "    def sigmoid_prime(self,X):\n",
        "        return X * (1 - X)\n",
        "        \n",
        "    def train(self, epochs):\n",
        "        \n",
        "        for e in range(epochs):\n",
        "            \n",
        "            #FORWARD PROPAGATION\n",
        "            \n",
        "            #hidden layer\n",
        "            # W1(398,30) X(30,12)\n",
        "            l1 = self.sigmoid(np.dot(self.X, self.w1))\n",
        "\n",
        "            #output layer\n",
        "            #l1(398,12) W2(12,1)\n",
        "            l2 = self.sigmoid(np.dot(l1, self.w2))\n",
        "        \n",
        "            # BACKPROPAGATION\n",
        "            \n",
        "            #calculate how far off our prediciton was\n",
        "            error = self.y-l2\n",
        "            \n",
        "            #calculate how far off each layer is\n",
        "            l2_delta = error * self.sigmoid_prime(l2)\n",
        "            l1_delta = l2_delta.dot(self.w2.T) * self.sigmoid_prime(l1)\n",
        "\n",
        "            #update weights with our newly found error values\n",
        "            self.w2 = np.add(self.w2, l1.T.dot(l2_delta) * self.learning_rate)\n",
        "            self.w1 = np.add(self.w1, self.X.T.dot(l1_delta) * self.learning_rate)\n",
        "        \n",
        "        #print('Error:', (abs(error)).mean())\n",
        "    \n",
        "    def test(self):\n",
        "        correct = 0\n",
        "        pred_list = []\n",
        "        \n",
        "        #replicate feedforward network for testing\n",
        "        l1 = self.sigmoid(np.dot(self.X_test, self.w1))\n",
        "        l2 = self.sigmoid(np.dot(l1, self.w2))\n",
        "        \n",
        "        #loop through all of the outputs of layer 2\n",
        "        for i in range(len(l2)):\n",
        "            if l2[i] >= 0.5:\n",
        "                pred = 1\n",
        "            else:\n",
        "                pred = 0\n",
        "\n",
        "            if pred == self.y_test[i]:\n",
        "                correct += 1\n",
        "                \n",
        "            pred_list.append(pred)\n",
        "        print(pred_list)\n",
        "        print(y_test)\n",
        "        correct = np.sum(pred_list == y_test)\n",
        "        print(\"%d out of %d predictions correct\" % (correct, len(pred_list)))\n",
        "\n",
        "        print(\"Test Accuracy: \", ((correct/len(y_test))*100),'%')\n",
        "        acc_list.append((correct/len(y_test))*100)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDS1zGi1SWHX",
        "outputId": "8987d597-39d0-4087-a19f-d63943ee06e5"
      },
      "source": [
        "nn = NeuralNetwork_sigmoid(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0]\n",
            "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "164 out of 171 predictions correct\n",
            "Test Accuracy:  95.90643274853801 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s08J_0DbsYL"
      },
      "source": [
        "class NeuralNetwork_tanh:\n",
        "    \n",
        "    def __init__(self,X, y, X_test, y_test, hidden_nodes=12, learning_rate=0.1, epochs=5000):\n",
        "        \n",
        "        #data\n",
        "        self.y = y[:,None]\n",
        "        self.X = X\n",
        "        \n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "\n",
        "        #parameters\n",
        "        np.random.seed(4)\n",
        "        self.input_nodes = len(X[0])\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = self.y.shape[1]\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        #init weights\n",
        "        self.w1 = 2*np.random.random((self.input_nodes, self.hidden_nodes)) - 1\n",
        "        self.w2 = 2*np.random.random((self.hidden_nodes, self.output_nodes)) - 1\n",
        "\n",
        "        self.train(epochs)\n",
        "        self.test()\n",
        "        \n",
        "    \"\"\"def sigmoid(self,X):\n",
        "        return (1/(1+np.exp(-X)))\"\"\"\n",
        "    def tanh(self,x):\n",
        "       return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "    def tanh_prime(self,x):\n",
        "      t = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "      return 1-t**2\n",
        "\n",
        "    \"\"\"def sigmoid_prime(self,X):\n",
        "        return X * (1 - X)\"\"\"\n",
        "        \n",
        "    def train(self, epochs):\n",
        "        \n",
        "        for e in range(epochs):\n",
        "            \n",
        "            #FORWARD PROPAGATION\n",
        "            \n",
        "            #hidden layer\n",
        "            # W1(398,30) X(30,12)\n",
        "            l1 = self.tanh(np.dot(self.X, self.w1))\n",
        "\n",
        "            #output layer\n",
        "            #l1(398,12) W2(12,1)\n",
        "            l2 = self.tanh(np.dot(l1, self.w2))\n",
        "        \n",
        "            # BACKPROPAGATION\n",
        "            \n",
        "            #calculate how far off our prediciton was\n",
        "            error = self.y-l2\n",
        "            \n",
        "            #calculate how far off each layer is\n",
        "            l2_delta = error * self.tanh_prime(l2)\n",
        "            l1_delta = l2_delta.dot(self.w2.T) * self.tanh_prime(l1)\n",
        "\n",
        "            #update weights with our newly found error values\n",
        "            self.w2 = np.add(self.w2, l1.T.dot(l2_delta) * self.learning_rate)\n",
        "            self.w1 = np.add(self.w1, self.X.T.dot(l1_delta) * self.learning_rate)\n",
        "        \n",
        "        #print('Error:', (abs(error)).mean())\n",
        "    \n",
        "    def test(self):\n",
        "        correct = 0\n",
        "        pred_list = []\n",
        "        \n",
        "        #replicate feedforward network for testing\n",
        "        l1 = self.tanh(np.dot(self.X_test, self.w1))\n",
        "        l2 = self.tanh(np.dot(l1, self.w2))\n",
        "        \n",
        "        #loop through all of the outputs of layer 2\n",
        "        for i in range(len(l2)):\n",
        "            if l2[i] >= 0.5:\n",
        "                pred = 1\n",
        "            else:\n",
        "                pred = 0\n",
        "\n",
        "            if pred == self.y_test[i]:\n",
        "                correct += 1\n",
        "                \n",
        "            pred_list.append(pred)\n",
        "        print(pred_list)\n",
        "        print(y_test)\n",
        "        correct = np.sum(pred_list == y_test)\n",
        "        print(\"%d out of %d predictions correct\" % (correct, len(pred_list)))\n",
        "\n",
        "        print(\"Test Accuracy: \", ((correct/len(y_test))*100),'%')\n",
        "        acc_list.append((correct/len(y_test))*100)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhHcVoWIeWTd",
        "outputId": "1d695f14-d66b-45b7-c687-00764e934fb2"
      },
      "source": [
        "nn = NeuralNetwork_tanh(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "108 out of 171 predictions correct\n",
            "Test Accuracy:  63.1578947368421 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZmwPJQeQ1Hy"
      },
      "source": [
        "class NeuralNetwork_relu:\n",
        "    \n",
        "    def __init__(self,X, y, X_test, y_test, hidden_nodes=12, learning_rate=0.1, epochs=5000):\n",
        "        \n",
        "        #data\n",
        "        self.y = y[:,None]\n",
        "        self.X = X\n",
        "        \n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "\n",
        "        #parameters\n",
        "        np.random.seed(4)\n",
        "        self.input_nodes = len(X[0])\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = self.y.shape[1]\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        #init weights\n",
        "        self.w1 = 2*np.random.random((self.input_nodes, self.hidden_nodes)) - 1\n",
        "        self.w2 = 2*np.random.random((self.hidden_nodes, self.output_nodes)) - 1\n",
        "\n",
        "        self.train(epochs)\n",
        "        self.test()\n",
        "        \n",
        "    \"\"\"def sigmoid(self,X):\n",
        "        return (1/(1+np.exp(-X)))\n",
        "    def tanh(self,x):\n",
        "       return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "    def tanh_prime(self,x):\n",
        "      t = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "      return 1-t**2\n",
        "\n",
        "    def sigmoid_prime(self,X):\n",
        "        return X * (1 - X)\"\"\"\n",
        "\n",
        "    def relu(self,X):\n",
        "      return np.maximum(0,X)\n",
        "    def relu_prime(self,X):\n",
        "      return np.maximum(0,1)\n",
        "        \n",
        "    def train(self, epochs):\n",
        "        \n",
        "        for e in range(epochs):\n",
        "            \n",
        "            #FORWARD PROPAGATION\n",
        "            \n",
        "            #hidden layer\n",
        "            # W1(398,30) X(30,12)\n",
        "            l1 = self.relu(np.dot(self.X, self.w1))\n",
        "\n",
        "            #output layer\n",
        "            #l1(398,12) W2(12,1)\n",
        "            l2 = self.relu(np.dot(l1, self.w2))\n",
        "        \n",
        "            # BACKPROPAGATION\n",
        "            \n",
        "            #calculate how far off our prediciton was\n",
        "            error = self.y-l2\n",
        "            \n",
        "            #calculate how far off each layer is\n",
        "            l2_delta = error * self.relu_prime(l2)\n",
        "            l1_delta = l2_delta.dot(self.w2.T) * self.relu_prime(l1)\n",
        "\n",
        "            #update weights with our newly found error values\n",
        "            self.w2 = np.add(self.w2, l1.T.dot(l2_delta) * self.learning_rate)\n",
        "            self.w1 = np.add(self.w1, self.X.T.dot(l1_delta) * self.learning_rate)\n",
        "        \n",
        "        #print('Error:', (abs(error)).mean())\n",
        "    \n",
        "    def test(self):\n",
        "        correct = 0\n",
        "        pred_list = []\n",
        "        \n",
        "        #replicate feedforward network for testing\n",
        "        l1 = self.relu(np.dot(self.X_test, self.w1))\n",
        "        l2 = self.relu(np.dot(l1, self.w2))\n",
        "        \n",
        "        #loop through all of the outputs of layer 2\n",
        "        for i in range(len(l2)):\n",
        "            if l2[i] >= 0.5:\n",
        "                pred = 1\n",
        "            else:\n",
        "                pred = 0\n",
        "\n",
        "            if pred == self.y_test[i]:\n",
        "                correct += 1\n",
        "                \n",
        "            pred_list.append(pred)\n",
        "        print(pred_list)\n",
        "        print(y_test)\n",
        "        correct = np.sum(pred_list == y_test)\n",
        "        print(\"%d out of %d predictions correct\" % (correct, len(pred_list)))\n",
        "\n",
        "        print(\"Test Accuracy: \", ((correct/len(y_test))*100),'%')\n",
        "        acc_list.append((correct/len(y_test))*100)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEMZj-zWRkeY",
        "outputId": "b261c897-13db-4874-e65c-571852e7b77d"
      },
      "source": [
        "nn = NeuralNetwork_relu(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "108 out of 171 predictions correct\n",
            "Test Accuracy:  63.1578947368421 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX1UunHYY0b_",
        "outputId": "d1823e07-102b-48b0-fac8-87ebeef02765"
      },
      "source": [
        "print(acc_list)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[95.90643274853801, 63.1578947368421, 63.1578947368421]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "vYB-k0NXZXcX",
        "outputId": "ac868bb0-cf2a-4fd2-88e4-ee49acd64d7d"
      },
      "source": [
        "height = acc_list\n",
        "bars = ('Logistic', 'Tanh', 'ReLU')\n",
        "y_pos = np.arange(len(bars))\n",
        "\n",
        "# Basic plot\n",
        "plt.bar(y_pos, height, color=(0.2, 0.4, 0.6, 0.6))\n",
        " \n",
        "# use the plt.xticks function to custom labels\n",
        "plt.xticks(y_pos, bars, color='orange', rotation=45, fontweight='bold', fontsize='17', horizontalalignment='right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhEDS-3WcgHf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}